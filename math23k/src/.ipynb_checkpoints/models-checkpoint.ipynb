{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderSeq(nn.Module):\n",
    "    ...\n",
    "    def forward(self,\n",
    "                input_seqs,\n",
    "                input_lengths,\n",
    "                batch_graph,\n",
    "                hidden=None):\n",
    "        # Note: we run this all at once (over multiple batches of multiple sequences)\n",
    "\n",
    "        # input_seqs:    [seq_len, batch_size]\n",
    "        # input_lengths: [batch_size]\n",
    "        # batch_graph:   [batch_size, 5, seq_len, seq_len]\n",
    "\n",
    "        embedded = self.embedding(input_seqs)  # S x B x E\n",
    "        embedded = self.em_dropout(embedded)\n",
    "        # embedded:   [seq_len, batch_size, embedding_size]\n",
    "\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        pade_hidden = hidden\n",
    "\n",
    "        pade_outputs, pade_hidden = self.gru_pade(packed, pade_hidden)\n",
    "        pade_outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(pade_outputs)\n",
    "        # pade_outputs: [seq_len,   batch_size, 2*hidden_size]\n",
    "        # pade_hidden:  [2*n_layer, batch_size,   hidden_size]\n",
    "\n",
    "        problem_output = pade_outputs[-1, :, :self.hidden_size] + pade_outputs[0, :, self.hidden_size:]\n",
    "        pade_outputs   = pade_outputs[ :, :, :self.hidden_size] + pade_outputs[:, :, self.hidden_size:]  # S x B x H\n",
    "        # problem_output: [batch_size, hidden_size]\n",
    "        # pade_outputs:   [seq_len, batch_size, hidden_size]\n",
    "\n",
    "        _, pade_outputs = self.gcn(pade_outputs, batch_graph)\n",
    "        # pade_outputs: [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        pade_outputs = pade_outputs.transpose(0, 1)\n",
    "        # pade_outputs: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "        return pade_outputs, problem_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1) Node Representation**  \n",
    "**Module:** BiLSTM neural network  \n",
    "**Output:**  \n",
    "$$H = \\{h_{1}, ..., h_{N}\\} \\in R^{N \\times d}, N = m + l$$\n",
    "$d$ denotes the dimension of hidden vectors  \n",
    "$m$ represents the number of words  \n",
    "$l$ represents the number of quantities  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std  = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff ,d_out, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph_Module(nn.Module):\n",
    "    def __init__(self):\n",
    "        # 4层GCN网络\n",
    "        self.graph = clones(module=GCN(in_feat_dim=indim,\n",
    "                                       nhid=hiddim,\n",
    "                                       out_feat_dim=self.d_k,\n",
    "                                       dropout=dropout),\n",
    "                            N=4)\n",
    "        \n",
    "        self.feed_foward = PositionwiseFeedForward(indim, hiddim, outdim, dropout)\n",
    "        self.norm = LayerNorm(outdim)\n",
    "\n",
    "    def forward(self, graph_nodes, graph):\n",
    "        # graph_nodes: [seq_len, batch_size, hidden_size]\n",
    "        # graph:       [batch_size, 5, seq_len, seq_len]\n",
    "        nbatches = graph_nodes.size(0)\n",
    "        mbatches = graph.size(0)\n",
    "        if nbatches != mbatches:\n",
    "            graph_nodes = graph_nodes.transpose(0, 1)\n",
    "        # graph_nodes: [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        # adj (batch_size, K, K): adjacency matrix\n",
    "\n",
    "        # graph.numel(): 返回数组中的元素个数\n",
    "        if not bool(graph.numel()):\n",
    "            adj = self.get_adj(graph_nodes)\n",
    "            adj_list = [adj, adj, adj, adj]\n",
    "        else:\n",
    "            adj = graph.float()\n",
    "            # adj: [batch_size, 5, seq_len, seq_len]\n",
    "            # adj[:, 1, :]: Quantity Comparison Graph\n",
    "            # adj[:, 4, :]: Quantity Cell       Graph\n",
    "            adj_list = [adj[:, 1, :], adj[:, 1, :], adj[:, 4, :], adj[:, 4, :]]\n",
    "\n",
    "        g_feature = tuple([l(graph_nodes, x) for l, x in zip(self.graph, adj_list)])\n",
    "        # g_feature: ([batch_size, seq_len, outdim], [batch_size, seq_len, outdim], [batch_size, seq_len, outdim], [batch_size, seq_len, outdim])\n",
    "        # hidden_size = outdim * 4\n",
    "        g_feature = self.norm(torch.cat(g_feature, dim=2)) + graph_nodes  # Norm & Add => Z => Z^\n",
    "        # g_feature: [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        graph_encode_features = self.feed_foward(g_feature) + g_feature   # Norm & Add => Z-\n",
    "        # graph_encode_features: [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        # adj: [batch_size, 5, seq_len, seq_len]\n",
    "        return adj, graph_encode_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graph Transformer:**  \n",
    "  \n",
    "**Graph_Module**:  \n",
    "**inputs:**  \n",
    "* adjacency matrices of multiple graphs $\\{A_{k}\\}_{k=1}^{K}$\n",
    "* initial node embeddings $H$\n",
    "* Quantity Comparison Graph $adj[:, 1, :]$\n",
    "* Quantity Cell Graph $adj[:, 4, :]$\n",
    "  \n",
    "**outputs**: \n",
    "* adjacency matrices of multiple graphs $\\{A_{k}\\}_{k=1}^{K}$\n",
    "* graph representation $z_{g}$\n",
    "  \n",
    "**model**:  \n",
    "* for each graph $\\{A_{k}\\}_{k=1}^{K}$, where $K=4$, concatenate GCN learning\n",
    "* each GCN output = $[batch\\_size, seq\\_len, outdim]$, and $outdim=hidden\\_size / 4$  \n",
    "$$Z = \\overset{K}{\\underset{k=1}{\\parallel}} GCN(A_{k}, H)$$\n",
    "* $\\parallel$ denote the concatenation of the K GCN heads.  \n",
    "* GCN layer = Layer Normalization layer + Residual Connection\n",
    "$$\\hat{Z} = Z + LayerNorm(Z)$$\n",
    "* Feed-Forward Network FFN(two layer feed-forward network with relu function between layers)  \n",
    "$$FFN(x) = max(0, x W_{f1} + b_{f1}) W_{f2} + b_{f2}$$\n",
    "* feed-forward network sub-layer\n",
    "$$\\bar{Z} = \\hat{Z} + LayerNorm(FFN(\\hat{Z}))$$\n",
    "* **(not mentioned)** min-pooling operation on all learned node representations, then fed into fully connected neural network(FC) to generate the graph representation\n",
    "$$Z_{g} = FC(MinPool(\\bar{Z}))$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        ...\n",
    "        self.gc1 = GraphConvolution(in_feat_dim, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, out_feat_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GCN**  \n",
    "  \n",
    "**Inputs:**  \n",
    "* adjacency matrix which represent the graph structure $A_{k}$\n",
    "* feature matrix which mean the input feature of all nodes $X$\n",
    "  \n",
    "**Outputs:**  \n",
    "* graph node feature\n",
    "  \n",
    "**model**:  \n",
    "$$GCN(A_{k}, X) = GConv_{2}(A_{k}, GConv_{1}(A_{k}, X))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "\n",
    "# Graph_Conv\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        # input: [batch_size, seq_len, in_features]\n",
    "        support = torch.matmul(input, self.weight)  # input * weight\n",
    "        # support: [batch_size, seq_len, out_features]\n",
    "        output  = torch.matmul(adj, support)  # adj * input * weight\n",
    "        # output: [batch_size, seq_len, out_features]\n",
    "\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GraphConv**  \n",
    "  \n",
    "**input:**  \n",
    "* graph node representations $X = input$\n",
    "* adjancency matrix $A_{k} = adj$\n",
    "    \n",
    "**output:**  \n",
    "* graph updated feature \n",
    "  \n",
    "**model:**  \n",
    "$$GConv(A_{k}, X) = relu(A_{k} X^{T} W_{gk})$$  \n",
    "其中，$W_{gk} \\in R^{d \\times d_{k}}$, where $d_k = d/K$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **generate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **merge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
