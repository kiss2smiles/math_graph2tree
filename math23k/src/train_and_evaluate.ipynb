{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CODE ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数中各参数的意义  \n",
    "  \n",
    "|**param**|**type**|**meaning**|\n",
    "|:---:|:---:|:---:|\n",
    "|**input_batch**|list | 原始文本中单词在词表中的索引 |\n",
    "|**input_length**    |list  | 原始文本中单词序列的长度 |\n",
    "|**target_batch**    |list  | 输出公式中的单词在词表中的索引 |\n",
    "|**target_length**    |list  | 输出公式中单词序列的长度 |\n",
    "|**nums_stack_batch** |list  | 原始文本中的重复数字在number_values list中的索引 |\n",
    "|**num_size_batch**   |list | 原始文本中的数字个数 |\n",
    "|**generate_nums**    |list  |  原始数据集中所包含的常数(1, 3.14) |\n",
    "|**num_pos**        |list  |  原始文本中的数字在原始文本中的位置 |\n",
    "|**batch_graph**     |numpy  | 建立好的Quantity Cell Graph 和 Quantity Comparison Graph |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tree():\n",
    "   ...\n",
    "   # input_var:    [seq_len, batch_size]\n",
    "   # input_length: [batch_size]\n",
    "   # batch_graph:  [batch_size, 5, seq_len, seq_len]\n",
    "   encoder_outputs, problem_output = encoder(input_seqs=input_var,\n",
    "                                             input_lengths=input_length,\n",
    "                                             batch_graph=batch_graph)\n",
    "   # encoder_outputs: [seq_len, batch_size, hidden_size]\n",
    "   # problem_output:  [         batch_size, hidden_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**encoder\\_outputs**: graph representation $Z_{g}$  \n",
    "**problem\\_output**: &nbsp;&nbsp;root node $n_{0}$ goal vector $q_{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tree():\n",
    "    ...\n",
    "    # encoder_outputs: [batch_size, seq_len, hidden_size]\n",
    "    all_nums_encoder_outputs = get_all_number_encoder_outputs(encoder_outputs=encoder_outputs,\n",
    "                                                              num_pos=num_pos,\n",
    "                                                              batch_size=batch_size,\n",
    "                                                              num_size=num_size,\n",
    "                                                              hidden_size=encoder.hidden_size)\n",
    "    # all_nums_encoder_outputs: [batch_size, num_size, hidden_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**number  embedding** &nbsp;&nbsp;取自encoder\\_outputs  \n",
    "**constant embedding** 取自nn.Embedding(Prediction)，且每次生成新的节点时都会更新  \n",
    "**operator embedding** 取自nn.Embedding(GenerateNode)，且每次生成新的节点时都会更新  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tree():\n",
    "    ...\n",
    "    node_stacks       = [[TreeNode(embedding=_, left_flag=False)] for _ in problem_output.split(1, dim=0)]  # [1, hidden_size]\n",
    "    embeddings_stacks = [[] for _ in range(batch_size)]\n",
    "    left_childs       = [None for _ in range(batch_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**node\\_stacks**: TreeNode(embedding, left\\_flag), 记录节点node中的 goal vector $q$  \n",
    "初始根节点 $n_{0}$ 为 goal vector $q_{0}$ = problem_output  \n",
    "  \n",
    "**embedding\\_stacks**: TreeEmbedding, 记录节点node之前的节点的subtree embedding $t$(list)  \n",
    "如果为操作符(非叶子节点),此时embedding\\_stacks添加operator的token embedding $e(y|P)$，并设置terminal=False  \n",
    "如果为操作数(左孩子节点),此时embedding\\_stacks添加operator的token embedding $e(y|P)$，并设置terminal=True  \n",
    "如果为操作数(右孩子节点),此时  \n",
    "&emsp;&emsp;初始化右孩子节点的subtree embedding $t_{r}$ 为token embedding $e(y|P)$  \n",
    "&emsp;&emsp;弹出左孩子节点(terminal=True)的subtree embedding $t_{l}$和根节点的subtree embedding $t$(parent t)  \n",
    "&emsp;&emsp;循环完成merge操作, 得到右孩子节点的最终subtree embedding $t_{r}$，并设置terminal=True  \n",
    "\n",
    "**left\\_childs**: 记录节点node中当前节点的subtree embedding $t$  \n",
    "如果为操作符(非叶子节点),此时left\\_childs输出为None  \n",
    "如果为操作数(左孩子节点),此时left\\_childs输出为左孩子节点的subtree embedding $t_l$  \n",
    "如果为操作数(右孩子节点),此时left\\_childs输出为右孩子节点的subtree embedding $t_r$    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tree():\n",
    "    ...\n",
    "    # encoder_outputs:          node representation(words)\n",
    "    # all_nums_encoder_outputs: node representation(numbers)\n",
    "\n",
    "    # encoder_outputs:          [seq_len,  batch_size, hidden_size]\n",
    "    # all_nums_encoder_outputs: [batch_size, num_size, hidden_size]\n",
    "    # padding_hidden:           [1,       hidden_size]\n",
    "    # seq_mask:                 [batch_size, seq_len]\n",
    "    # num_mask:                 [batch_size, num_size + constant_size]\n",
    "    num_score, op, current_embeddings, current_context, current_nums_embeddings = predict(\n",
    "        node_stacks=node_stacks,\n",
    "        left_childs=left_childs,\n",
    "        encoder_outputs=encoder_outputs,\n",
    "        num_pades=all_nums_encoder_outputs,\n",
    "        padding_hidden=padding_hidden,\n",
    "        seq_mask=seq_mask,\n",
    "        mask_nums=num_mask)\n",
    "    outputs = torch.cat((op, num_score), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**num\\_score**: &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\\[batch\\_size, num\\_size + constant\\_size\\]  \n",
    "**op(op_score)**: &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\\[batch\\_size, operator\\_size\\]  \n",
    "**(GOAL VECTOR $q$)**  \n",
    "**current_embeddings**: &emsp;&emsp;&emsp;&nbsp;\\[batch\\_size, 1, hidden\\_size\\]  \n",
    "**(CONTEXT VECTOR $c$)**  \n",
    "**current_context**: &emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;\\[batch\\_size, 1, hidden\\_size\\]  \n",
    "**(CURRENT NUMBER EMBEDDING MATRIX $M_{num}$)**  \n",
    "**current_nums_embeddings**: \\[batch\\_size, num\\_size + constant\\_size, hidden\\_size\\]  \n",
    "  \n",
    "**(OUTPUT TOKEN LOGIT)**  \n",
    "**outputs:** &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;\\[batch\\_size, num\\_size + constant\\_size + operator\\_size\\]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tree():\n",
    "    ... \n",
    "    # 预测出每一个target的值\n",
    "    target_t, generate_input = generate_tree_input(target=target[t].tolist(),\n",
    "                                                   decoder_output=outputs,\n",
    "                                                   nums_stack_batch=nums_stack_batch,\n",
    "                                                   num_start=num_start,\n",
    "                                                   unk=unk)\n",
    "    # target_t:       [batch_size] = ground_truth\n",
    "    # generate_input: [batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注**: 上述代码只在预测token为操作符时有效  \n",
    "**target\\_t**: ground_truth token index  \n",
    "&emsp;&emsp;如果有重复数字，则取概率score最大的token index作为num_pos  \n",
    "&emsp;&emsp;如果有重复数字，则选择重复数字中概率最大的位置作为公式中重复数字的num_pos  \n",
    "**generate\\_input**: 操作符的token index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **generate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tree():\n",
    "    ...\n",
    "    # current_embeddings: [batch_size, 1, hidden_size]\n",
    "    # generate_input:     [batch_size]\n",
    "    # current_context:    [batch_size, 1, hidden_size]\n",
    "    left_child, right_child, node_label = generate(node_embedding=current_embeddings,\n",
    "                                                   node_label=generate_input,\n",
    "                                                   current_context=current_context)\n",
    "    # left_child:  [batch_size,    hidden_size]\n",
    "    # right_child: [batch_size,    hidden_size]\n",
    "    # node_label:  [batch_size, embedding_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**node\\_embeddings**:&nbsp;parent goal vector $q$  \n",
    "**current\\_context**:&emsp;&nbsp;&nbsp;parent context vector $c$  \n",
    "**node_label**: &emsp;&emsp;&emsp;&nbsp;&nbsp;parent token_embedding $e(\\hat{y}|P)$\n",
    "  \n",
    "$$o_{l} = \\sigma(W_{ol}[q, c, e(\\hat{y}|P)])$$  \n",
    "$$C_{l} = tanh(W_{cl}[q, c, e(\\hat{y}|P)])$$  \n",
    "$$h_{l} = o_{l} \\odot C_{l}$$  \n",
    "**left\\_child**:  当前node的left child的$h_l$  \n",
    "  \n",
    "$$o_{r} = \\sigma(W_{or}[q, c, e(\\hat{y}|P)])$$  \n",
    "$$C_{r} = tanh(W_{cr}[q, c, e(\\hat{y}|P)])$$  \n",
    "$$h_{r} = o_{r} \\odot C_{r}$$  \n",
    "**right\\_child**: 当前node的right child的$h_r$  \n",
    "**node\\_label**:  操作符的token\\_embedding $e(\\hat{y}|P)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **merge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tree():\n",
    "    ...\n",
    "    # op.embedding:        [1, embedding_size]\n",
    "    # sub_stree.embedding: [1,    hidden_size]\n",
    "    # current_num:         [1,    hidden_size]\n",
    "\n",
    "    current_num = merge(node_embedding=op.embedding,\n",
    "                        sub_tree_1=sub_stree.embedding,\n",
    "                        sub_tree_2=current_num)\n",
    "    # current_num: [1, hidden_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当前预测的token为操作数(叶子节点)时，更新叶子节点的Tree embedding  \n",
    "&emsp;&emsp;如果此时为右孩子节点，则通过左孩子节点的 subtree embedding $t_{l}$ 和 右孩子节点的subtree embedding $t_{r}$, 来更新根节点的subtree embedding $t$  \n",
    "  \n",
    "**op.embedding**:&emsp;&emsp;&emsp;&nbsp;&nbsp;&nbsp;parent node token embedding $e(\\hat{y}|P)$  \n",
    "**sub_stree.embedding**:&nbsp;left\\_sub\\_tree\\_embedding $t_{l}$  \n",
    "**current_num**:&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;right\\_sub\\_tree\\_embedding $t_{r}$  \n",
    "  \n",
    "$$g_{t} = \\sigma(W_{gt} [t_{l}, t_{r}, e(\\hat{y}|P)])$$  \n",
    "$$C_{t} = tanh  (W_{ct} [t_{l}, t_{r}, e(\\hat{y}|P)])$$  \n",
    "$$comb(t_{l}, t_{r}, \\hat{y}) = g_{t} \\odot C_{t}$$  \n",
    "$$t = comb(t_{l}, t_{r}, \\hat{y})$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
